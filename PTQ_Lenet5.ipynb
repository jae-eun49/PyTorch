{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c688ae26-bef9-4988-9133-0346dc4eff32",
   "metadata": {},
   "source": [
    "# Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5075e9a-43ff-49f5-888b-342d5ecda78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import tqdm\n",
    "from tqdm import tqdm_notebook as tq\n",
    "import os, time, math, copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_printoptions(precision=8, linewidth=50000)\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff365a5-2a3c-4c10-96db-c2d798de3517",
   "metadata": {},
   "source": [
    "# Print Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8afcf2b-07ca-4b03-93e7-89634b8b20e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLACK\t= '\\033[30m'\n",
    "RED\t\t= '\\033[31m'\n",
    "GREEN\t= '\\033[32m'\n",
    "YELLOW\t= '\\033[33m'\n",
    "BLUE\t= '\\033[34m'\n",
    "MAGENTA\t= '\\033[35m'\n",
    "CYAN\t= '\\033[36m'\n",
    "RESET\t= '\\033[0m'\n",
    "SEL\t\t= '\\033[7m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d6f449a-b38f-449a-abfc-dbdfffd65a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class\tfxp:\n",
    "\tdef\t__init__(self, bIn, iBWF):\n",
    "\t\tself.iFullBW\t= len(bIn)\n",
    "\t\tself.iIntgBW\t= self.iFullBW - iBWF\n",
    "\t\tself.bSign\t\t= bIn[0]\n",
    "\t\tself.bIntg\t\t= bIn[:self.iIntgBW]\n",
    "\t\tself.bFrac\t\t= bIn[self.iIntgBW:]\n",
    "\t\tself.fFull\t\t= 0\n",
    "\t\ttry:\n",
    "\t\t\tfor idx, bit in enumerate(bIn):\n",
    "\t\t\t\tif\tidx == 0:\n",
    "\t\t\t\t\tself.fFull = self.fFull + int(bit,2) * -pow(2, self.iIntgBW - 1)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tself.fFull = self.fFull + int(bit,2) * pow(2, self.iIntgBW - 1 - idx)\n",
    "\t\texcept:\n",
    "\t\t\tprint(bIn)\n",
    "\t\tself.dispFull\t= RED + self.bIntg + BLUE + self.bFrac + RESET\n",
    "\t\treturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "610b93d9-8619-4bcc-b29d-9a3d35104bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class\tflp2fix:\n",
    "\tdef\t__init__(self, fIn, iBW, iBWF):\n",
    "\t\tself.fMin\t\t= - 2 ** (iBW - iBWF - 1)\n",
    "\t\tself.fMax\t\t= (2 ** (iBW-1) - 1) * (2 ** -iBWF)\n",
    "\t\tself.fResol\t\t= 2 ** -iBWF\n",
    "\t\tif fIn < self.fMin or fIn > self.fMax:\n",
    "\t\t\tprint(f'({fIn}): Out of input range ({self.fMax}/{self.fMin}) during flp -> fix converting ')\n",
    "\t\tself.iBW\t\t= iBW\n",
    "\t\tself.iBWI\t\t= iBW - iBWF\n",
    "\t\tself.iBWF\t\t= iBWF\n",
    "\n",
    "\t\tself.iFLP2INT\t= abs(int(fIn * 2 ** iBWF))\n",
    "\t\tif fIn < 0:\n",
    "\t\t\tself.iFLP2INT = 2 ** (iBW-1) - self.iFLP2INT\n",
    "\n",
    "\t\tif fIn >= 0:\n",
    "\t\t\tself.bFull = bin(self.iFLP2INT)[2:].rjust(iBW, '0')\n",
    "\t\telse:\n",
    "\t\t\tself.bFull = '1'+bin(self.iFLP2INT)[2:].rjust(iBW-1, '0')\n",
    "\t\t\tif len(self.bFull) > iBW:\n",
    "\t\t\t\tself.bFull = '0' * iBW\n",
    "\n",
    "\t\tself.cssFxp\t\t= fxp(self.bFull, self.iBWF)\n",
    "\t\tself.bSign\t\t= self.cssFxp.bSign\n",
    "\t\tself.bIntg\t\t= self.cssFxp.bIntg\n",
    "\t\tself.bFrac\t\t= self.cssFxp.bFrac\n",
    "\t\tself.fFull\t\t= self.cssFxp.fFull\n",
    "\t\treturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48099f0b-1b79-482e-8ec7-e978b1909645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def\tflp2fixTensor(fIn, iBW, iBWF):\n",
    "# \tfMin = - 2 ** (iBW - iBWF - 1)\n",
    "# \tfMax = (2 ** (iBW-1) - 1) * (2 ** -iBWF)\n",
    "# \tfTensor = fIn * (2 ** iBWF)\n",
    "# \tfTensor = fTensor.round() * (2 ** -iBWF)\n",
    "# \tif (fTensor.min() < fMin or fMax < fTensor.max()):\n",
    "# \t\tprint(f'fMin: {fMin}, fMax: {fMax}, fTensor.Min:{fTensor.min()}, fTensor.Max:{fTensor.max()}')\n",
    "# \treturn fTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db9ba737-a911-4652-a369-267585a231c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def\tflp2fixTensor(fIn, iBW, iBWF):\n",
    "\tfMin = - 2 ** (iBW - iBWF - 1)\n",
    "\tfMax = (2 ** (iBW-1) - 1) * (2 ** -iBWF)\n",
    "\tfList = []\n",
    "\tfor aTensor in fIn.view(-1):\n",
    "\t\tfList.append(flp2fix(aTensor, iBW, iBWF).fFull)\n",
    "\treturn torch.tensor(fList).view(fIn.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b68ddb-8ef2-42a1-94c1-4f708b4907d7",
   "metadata": {},
   "source": [
    "# User Define Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "470d6312-f557-4f84-899f-95686b41e1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '~/dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002c0a67-1a39-4ce3-913b-749c11f6878b",
   "metadata": {},
   "source": [
    "# Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa129a9b-9a85-4de6-b42a-f8cbfc31231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch for MNIST dataset')\n",
    "parser.add_argument('--device', type=str, default='cpu', help='Device')\n",
    "parser.add_argument('--shuffle', action='store_true', default=False, help='enables data shuffle')\n",
    "parser.add_argument('--dataset', type=str, default='mnist', help='training dataset')\n",
    "parser.add_argument('--data_path', type=str, default=data_path, help='path to MNIST')\n",
    "parser.add_argument('--batch_size', type=int, default=64, help='batch size')\n",
    "parser.add_argument('--epochs', type=int, default=10, help='number of epochs to train')\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='learning rate')\n",
    "parser.add_argument('--optimizer', type=str, default='adam', help='optimizer')\n",
    "parser.add_argument('--loss_func', type=str, default='cel', help='optimizer')\n",
    "parser.add_argument('--quant_opt', type=str, default='asym', help='Type of Quantization')\n",
    "parser.add_argument('--full_bits', type=int, default=16, help='Number of Quantization Bits')\n",
    "parser.add_argument('--frac_bits', type=int, default=8, help='Number of Quantization Bits')\n",
    "parser.add_argument('--pretrained', type=bool, default=True, help='Pretrained Model')\n",
    "parser.add_argument('--act_quant', type=bool, default=False, help='Activation Quantization')\n",
    "parser.add_argument('--disp', type=bool, default=False, help='Display Model Information')\n",
    "\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87afa21c-b407-4698-bf0b-85e9dc4c94f1",
   "metadata": {},
   "source": [
    "# Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c82ac1a-0065-4d3f-a915-a01a91eb47a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.device == 'cuda' else {}\n",
    "data_transform = transforms.Compose([transforms.Resize((32,32)),transforms.ToTensor()])\n",
    "if args.dataset == 'mnist':\n",
    "\ttrain_loader = torch.utils.data.DataLoader(\n",
    "\t\tdataset=datasets.MNIST(\n",
    "\t\t\troot=args.data_path,\n",
    "\t\t\ttrain=True,\n",
    "\t\t\tdownload=True,\n",
    "\t\t\ttransform=data_transform\n",
    "\t\t),\n",
    "\t\tbatch_size=args.batch_size,\n",
    "\t\tshuffle=args.shuffle,\n",
    "\t\t**kwargs\n",
    "\t)\n",
    "\n",
    "\ttest_loader = torch.utils.data.DataLoader(\n",
    "\t\tdataset=datasets.MNIST(\n",
    "\t\t\troot=args.data_path,\n",
    "\t\t\ttrain=False,\n",
    "\t\t\tdownload=True,\n",
    "\t\t\ttransform=data_transform\n",
    "\t\t),\n",
    "\t\tbatch_size=args.batch_size,\n",
    "\t\tshuffle=args.shuffle,\n",
    "\t\t**kwargs\n",
    "\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625782c6-b278-4485-a8c9-7c6fec5aafdf",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98354784-1637-4f2c-85b2-6b53f0805bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class MLP(nn.Module):\n",
    "#\tdef __init__(self):\n",
    "#\t\tsuper(MLP, self).__init__()\n",
    "#\t\tself.flatten = nn.Flatten()\n",
    "#\t\tself.fc1 = nn.Linear(28*28, 16)\n",
    "#\t\tself.relu1 = nn.ReLU()\n",
    "#\t\tself.fc2 = nn.Linear(16, 16)\n",
    "#\t\tself.relu2 = nn.ReLU()\n",
    "#\t\tself.fc3 = nn.Linear(16, 10)\n",
    "\t\t\n",
    "#\tdef forward(self, x):\n",
    "#\t\tx = self.flatten(x)\n",
    "#\t\tx = self.fc1(x)\n",
    "#\t\tx = self.relu1(x)\n",
    "#\t\tx = self.fc2(x)\n",
    "#\t\tx = self.relu2(x)\n",
    "#\t\tlogits = self.fc3(x)\n",
    "#\t\treturn logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fe7be2b-c72c-4c32-881a-e32b82f3345f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet_5(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc2): Linear(in_features=84, out_features=10, bias=True)\n",
      "  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (act): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LeNet_5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet_5,self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1)\n",
    "        self.conv3 = nn.Conv2d(16, 120, kernel_size=5, stride=1)  # Flatten layer\n",
    "        self.fc1 = nn.Linear(120, 84)\n",
    "        self.fc2 = nn.Linear(84, 10)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.avg_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.avg_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.flatten(x)\n",
    "#        x = x.view(-1, 120) # Flatten layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = LeNet_5()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ebfd9aa-9e7b-4638-8db1-101093cdfb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genOptimizer(model, args):\n",
    "\tif args.optimizer == 'sgd':\n",
    "\t\toptimizer = torch.optim.SGD(model.parameters(), lr=args.lr)\n",
    "\tif args.optimizer == 'adam':\n",
    "\t\toptimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\treturn optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b9716a0-8033-456a-89d5-36af18bbffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genLossFunc(args):\n",
    "\tif args.loss_func == 'cel':\n",
    "\t\tloss_func = nn.CrossEntropyLoss()\n",
    "\treturn loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb8fe687-70b1-4f29-8dd4-316acae35dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, epoch, args):\n",
    "    model.train()\n",
    "    loss_func = genLossFunc(args)\n",
    "    optimizer = genOptimizer(model, args)\n",
    "    max_batch_index = int(np.floor(len(train_loader.dataset)/args.batch_size)) #batch 번호 ..?\n",
    "    running_loss,correct = 0, 0\n",
    "    \n",
    "    for batch_index, (image, label) in enumerate(tq(train_loader, desc='Train', leave=False)):\n",
    "        image, label = image.to(args.device), label.to(args.device)\n",
    "        pred = model(image) #pred = model\n",
    "        loss = loss_func(pred, label) # model 이용해서 loss 구함)\n",
    "        running_loss += loss.item()#*image.size(0)\n",
    "        correct += (pred.argmax(1) == label).type(torch.int).sum().item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    final_loss = running_loss/len(train_loader.dataset)\n",
    "    correct_rate = 100 * correct / len(train_loader.dataset)\n",
    "    print(f'Epoch {epoch+1:<3d}: Avg. Loss: {final_loss:.4f}', end = '\\t')\n",
    "    print(f'Accuracy: {correct}/{len(train_loader.dataset)} ({correct_rate:>.1f}%)')\n",
    "    \n",
    "    return final_loss,correct_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29189102-050a-463a-bbb8-18ca4ab3700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, model, args):\n",
    "\tmodel.eval()\n",
    "\twith torch.no_grad():\n",
    "\t\tloss_func = genLossFunc(args)\n",
    "\t\tloss, correct = 0, 0\n",
    "# \t\tfor batch_index, (image, label) in enumerate(tq(test_loader, desc='Test', leave=False)):\n",
    "\t\tfor batch_index, (image, label) in enumerate(test_loader):\n",
    "\t\t\timage, label = image.to(args.device), label.to(args.device)\n",
    "\t\t\tpred = model(image)\n",
    "\t\t\tloss += loss_func(pred, label).item()#*image.size(0)\n",
    "\t\t\tcorrect += (pred.argmax(1) == label).type(torch.int).sum().item()\n",
    "\tloss /= len(test_loader.dataset)\n",
    "\tcorrect_rate = 100 * correct / len(test_loader.dataset)\n",
    "\tprint(f'Accuracy: {correct}/{len(test_loader.dataset)} ({correct_rate:>.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec4a2872-aab5-491a-8b9c-53861468e10d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  : Avg. Loss: 0.0054\tAccuracy: 53740/60000 (89.6%)\n",
      "Accuracy: 9523/10000 (95.2%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  : Avg. Loss: 0.0017\tAccuracy: 58073/60000 (96.8%)\n",
      "Accuracy: 9679/10000 (96.8%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3  : Avg. Loss: 0.0011\tAccuracy: 58717/60000 (97.9%)\n",
      "Accuracy: 9784/10000 (97.8%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4  : Avg. Loss: 0.0008\tAccuracy: 59067/60000 (98.4%)\n",
      "Accuracy: 9818/10000 (98.2%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5  : Avg. Loss: 0.0006\tAccuracy: 59281/60000 (98.8%)\n",
      "Accuracy: 9840/10000 (98.4%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6  : Avg. Loss: 0.0005\tAccuracy: 59440/60000 (99.1%)\n",
      "Accuracy: 9843/10000 (98.4%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7  : Avg. Loss: 0.0004\tAccuracy: 59563/60000 (99.3%)\n",
      "Accuracy: 9813/10000 (98.1%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8  : Avg. Loss: 0.0003\tAccuracy: 59613/60000 (99.4%)\n",
      "Accuracy: 9860/10000 (98.6%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9  : Avg. Loss: 0.0003\tAccuracy: 59676/60000 (99.5%)\n",
      "Accuracy: 9849/10000 (98.5%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 : Avg. Loss: 0.0002\tAccuracy: 59698/60000 (99.5%)\n",
      "Accuracy: 9860/10000 (98.6%)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def main(model):\n",
    "\tfor epoch in range(args.epochs):\n",
    "\t\ttrain(train_loader, model, epoch, args)\n",
    "\t\ttest(test_loader, model, args)\n",
    "\tprint(\"Done!\")\n",
    "\treturn model\n",
    "\n",
    "model = main(LeNet_5().to(args.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f33b1a6e-c60e-4778-8764-6d56ef45b8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 9667/10000 (96.7%)\n"
     ]
    }
   ],
   "source": [
    "if args.pretrained:\n",
    "\tmodel = LeNet_5().to(args.device)\n",
    "\tmodel.load_state_dict(torch.load('PTQ_LeNet.pth'))\n",
    "\ttest(test_loader, model, args)\n",
    "else:\n",
    "\tmodel = main(LeNet_5().to(args.device))\n",
    "\ttorch.save(model.state_dict(), 'PTQ_LeNet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cd9ecd0-0247-4bde-bbb9-2a5c2478c246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model2fix(model, args):\n",
    "\tfor name, _ in model.named_parameters():\n",
    "\t\texec(f'model.{name}.data = flp2fixTensor(model.{name}.data, {args.full_bits}, {args.frac_bits})')\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "456938c0-6707-4718-85e2-0895fd6e0f22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def in2fix(images, full_width=args.full_bits, frac_width=args.frac_bits):\n",
    "# \tdim_images = images.size()\n",
    "# \timages = images.view(-1)\n",
    "# \tfor idx_image, image in enumerate(images):\n",
    "# \t\ttemp_css = flp2fix(image, full_width, frac_width)\n",
    "# \t\timages[idx_image] = torch.tensor(temp_css.fFull)\n",
    "# \t\tdel temp_css\n",
    "# \treturn images.view(dim_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebfd12bd-736c-4f63-af51-25495477c998",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def quantFixForward(model, x, args):\n",
    "    cmodel = copy.deepcopy(model).to(args.device)\n",
    "    with torch.no_grad():\n",
    "\n",
    "        act0 = cmodel.pool(cmodel.act(cmodel.conv1(x)))\n",
    "        act0 = flp2fixTensor(act0, args.full_bits, args.frac_bits)\n",
    "        \n",
    "        act1 = cmodel.pool(cmodel.act(cmodel.conv2(act0)))\n",
    "        act1 = flp2fixTensor(act1, args.full_bits, args.frac_bits)\n",
    "        \n",
    "        act2 = cmodel.act(cmodel.conv3(act1))\n",
    "        act2 = flp2fixTensor(act2, args.full_bits, args.frac_bits)\n",
    "\n",
    "        act3 = cmodel.flatten(act2)\n",
    "        act3 = flp2fixTensor(act3, args.full_bits, args.frac_bits)\n",
    "\t\t\n",
    "        act4 = cmodel.act(cmodel.fc1(act3))\n",
    "        act4 = flp2fixTensor(act4, args.full_bits, args.frac_bits)\n",
    "        \n",
    "        act5 = cmodel.fc2(act4)\n",
    "        act5 = flp2fixTensor(act5, args.full_bits, args.frac_bits)\n",
    "        \n",
    "    return cmodel, act0, act1, act2, act3, act4, act5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3d35ae3-970e-4386-9e03-86690e48af4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testQuant(model, test_loader, args):\n",
    "\tqmodel = copy.deepcopy(model).to(args.device)\n",
    "\tqmodel = model2fix(qmodel, args)\n",
    "\tqmodel.eval()\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tloss_func = genLossFunc(args)\n",
    "\t\tloss, correct = 0, 0\n",
    "\t\tfor batch_index, (image, label) in enumerate(test_loader):\n",
    "\t\t\timage, label = image.to(args.device), label.to(args.device)\n",
    "\t\t\tqmodel, act0, act1, act2, act3, act4, act5  = quantFixForward(qmodel, image, args)\n",
    "\t\t\ty = act5\n",
    "\t\t\tloss += loss_func(y, label).item()#*image.size(0)\n",
    "\t\t\tcorrect += (y.argmax(1) == label).type(torch.int).sum().item()\n",
    "\tcorrect_rate = 100 * correct / len(test_loader.dataset)\n",
    "\tprint(f'Accuracy: {correct}/{len(test_loader.dataset)} ({correct_rate:>.1f}%) Loss: {loss/len(test_loader.dataset):.2f}')\n",
    "\treturn qmodel, act0, act1, act2, act3, act4, act5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cab11ff5-8eea-4928-85a9-b92145de83d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 9661/10000 (96.6%) Loss: 0.00\n"
     ]
    }
   ],
   "source": [
    "qmodel, act0, act1, act2, act3, act4, act5 = testQuant(model, test_loader, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f2ffe05-2a11-431d-acae-ba497de3f53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractParams(model, args):\n",
    "    for key in model.state_dict().keys():\n",
    "        layer_name = key.split('.')[0]\n",
    "        param_type = 'w' if 'weight' in key else 'b'\n",
    "        for idx, params in enumerate(eval(f'qmodel.{key}.data')):\n",
    "        #    print(idx)\n",
    "        #    print(params.dim())\n",
    "            with open(f'./mif/{layer_name}_{param_type}_{idx}.mif', 'w') as fh:\n",
    "                if param_type == 'w':\n",
    "                    if params.dim() == 3 :\n",
    "                        for idx, dim1 in enumerate(params):\n",
    "                            for idx, dim2 in enumerate(dim1):\n",
    "                                for idx, param in enumerate(dim2):\n",
    "                                    bin_param = flp2fix(param, args.full_bits, args.frac_bits).bFull\n",
    "                                    fh.write(bin_param + ('\\n','')[idx == len(params)-1])\n",
    "                    elif params.dim() == 1: # fc2 layer\n",
    "                        for idx, param in enumerate(params):\n",
    "                            bin_param = flp2fix(param, args.full_bits, args.frac_bits).bFull\n",
    "                            fh.write(bin_param + ('\\n', '')[idx == len(params)-1])\n",
    "                    \n",
    "                #    for idx, param in enumerate(params):\n",
    "                #        bin_param = flp2fix(param, args.full_bits, args.frac_bits).bFull\n",
    "                #        fh.write(bin_param + ('\\n','')[idx == len(params)-1])\n",
    "                else:\n",
    "                    bin_param = flp2fix(params, args.full_bits, args.frac_bits).bFull\n",
    "                    fh.write(bin_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42eabfcc-a77d-4f0a-ad83-13d56f42a664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genInputVector(test_loader, args):\n",
    "\tout_path = './vec'\n",
    "\tos.system(f'rm -rf {out_path};mkdir -p {out_path}')\n",
    "\twith open(f'{out_path}/labels.vec', 'w') as fh_labels:\n",
    "\t\twith open(f'{out_path}/images.vec', 'w') as fh_images:\n",
    "\t\t\tfor batch_index, (images, labels) in enumerate(test_loader):\n",
    "\t\t\t\tfor (image, label) in zip(images, labels):\n",
    "\t\t\t\t\tbin_label = flp2fix(label, args.full_bits, 0).bFull\n",
    "\t\t\t\t\tfh_labels.write(bin_label+'\\n')\n",
    "\t\t\t\t\tfor pixel in image.view(-1):\n",
    "\t\t\t\t\t\tbin_pixel = flp2fix(pixel, args.full_bits, args.frac_bits).bFull\n",
    "\t\t\t\t\t\tfh_images.write(bin_pixel+'\\n')\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85911b39-7f25-4e1f-8faa-bd3d8632e6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if not args.pretrained:\n",
    "extractParams(model, args)\n",
    "genInputVector(test_loader, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8310847e-5a81-4649-a092-86466d06bdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = datasets.MNIST(\troot=args.data_path,\n",
    "\t\t\t\t\t\t\ttrain=False,\n",
    "\t\t\t\t\t\t\tdownload=True,\n",
    "\t\t\t\t\t\t\ttransform=data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9cc749c5-67ad-4d00-8512-8e7f4390716f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: /home/jaeeun/dataset\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(32, 32), interpolation=PIL.Image.BILINEAR)\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "702beea0-7a66-4f44-b0b8-5e9dc5314bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0][0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "250d7de6-54f2-4019-bf3b-ecea23df6462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 28, 28])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0f5e566-ed4d-472b-ac43-3586b95f5e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = iter(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2627d86d-b7d6-4449-ba38-567a9eb85cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labs = next(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e490a48b-9c69-4e1b-99bf-64daf58694a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5, 4, 0, 7, 4, 0, 1, 3, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labs[0:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d4073376-555b-4486-8106-e6074e0671a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0330ba5d-d615-4a11-820c-914ec3ad16f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.01562500, -0.06640625,  0.03515625, -0.01171875,  0.08203125, -0.12109375, -0.01171875,  0.11328125], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qmodel.fc1.weight[0][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "71494c29-cd7d-4fa2-b543-f3ca1435ee35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 3.45630556e-01,  4.49961007e-01,  5.31258941e-01,  5.59453726e-01,  5.24021506e-01],\n",
       "          [ 4.85830247e-01,  6.16211116e-01,  7.02184618e-01,  7.14567602e-01,  6.50589764e-01],\n",
       "          [ 5.66228449e-01,  6.98803782e-01,  7.70675838e-01,  7.58951485e-01,  6.70978010e-01],\n",
       "          [ 5.50445318e-01,  6.61313534e-01,  7.06644952e-01,  6.71979725e-01,  5.74740529e-01],\n",
       "          [ 4.43041593e-01,  5.17175198e-01,  5.33528864e-01,  4.87122267e-01,  4.02040064e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 9.32341293e-02,  2.15957433e-01, -8.39374810e-02,  2.11399645e-01, -4.80696484e-02],\n",
       "          [ 1.70069352e-01,  3.84582691e-02,  1.41682699e-01,  1.15408339e-01, -1.47190243e-01],\n",
       "          [-4.23519462e-02,  1.38037458e-01,  2.24406019e-01, -8.58325958e-02, -4.56176996e-02],\n",
       "          [ 6.56774044e-02,  1.80821389e-01,  1.64696380e-01, -4.18815576e-02,  9.94063169e-02],\n",
       "          [ 8.67636204e-02, -3.69882733e-02, -1.14126392e-01,  3.44202444e-02, -7.69143831e-03]]],\n",
       "\n",
       "\n",
       "        [[[-9.13127065e-02,  1.54354632e-01, -2.42399741e-02,  1.00654304e-01, -1.31763414e-01],\n",
       "          [-3.57904062e-02,  1.67974666e-01,  2.14750141e-01, -1.46772191e-01,  1.61215723e-01],\n",
       "          [ 6.66750744e-02, -1.50744244e-02, -8.73405337e-02,  1.58600152e-01,  1.89641923e-01],\n",
       "          [-1.69372171e-01,  1.60933957e-01,  1.91613972e-01, -3.97968777e-02,  1.86211288e-01],\n",
       "          [-1.06873848e-01,  3.83788794e-02,  1.66823938e-01,  7.79376850e-02,  1.82986498e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.11662364e-02,  3.22596319e-02, -1.62536979e-01,  6.20162077e-02, -1.51143551e-01],\n",
       "          [ 7.09114037e-03,  3.54913659e-02,  9.25822258e-02,  1.68724582e-01,  1.47554457e-01],\n",
       "          [-2.89912596e-02, -1.14685036e-01, -1.13152549e-01, -1.99071348e-01, -1.04124516e-01],\n",
       "          [ 7.05328807e-02, -1.74327090e-01, -3.80990617e-02,  1.12645537e-01,  1.85963511e-02],\n",
       "          [ 1.68213323e-01,  2.37959754e-02, -1.94360107e-01, -3.47525603e-03,  7.63403252e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 7.38661224e-03,  9.89799425e-02, -1.88829288e-01,  1.84024736e-01, -1.24140076e-01],\n",
       "          [ 2.27126144e-02,  6.75330982e-02,  1.68043107e-01, -5.16063012e-02, -1.35360926e-01],\n",
       "          [-1.78722039e-01, -1.62010521e-01, -3.86295244e-02,  8.26329291e-02, -2.01596394e-01],\n",
       "          [ 1.67876467e-01,  8.41126591e-02,  8.22473690e-02,  5.03196009e-02, -2.80705541e-02],\n",
       "          [ 1.42953724e-01,  1.72427073e-01, -3.84405702e-02, -4.62395363e-02, -2.69762357e-04]]],\n",
       "\n",
       "\n",
       "        [[[-1.73332781e-01,  1.50642514e-01,  1.09267913e-01,  1.96874663e-02,  2.07344711e-01],\n",
       "          [-1.56664401e-01, -3.55038955e-03, -5.91975525e-02,  1.71430781e-01, -1.43895924e-01],\n",
       "          [-4.13134694e-02,  1.04147926e-01,  1.07894503e-01, -2.18640231e-02, -1.57576516e-01],\n",
       "          [ 5.56434318e-02, -1.22257195e-01, -9.24563706e-02, -1.02997564e-01,  1.37589335e-01],\n",
       "          [-1.40158981e-01, -1.34463757e-01,  7.34237731e-02,  6.98443502e-02,  1.64201900e-01]]]], requires_grad=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "99b2a232-9693-4ebb-a46a-2c5096c59fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.34375000, 0.44921875, 0.53125000, 0.55859375, 0.52343750],\n",
       "        [0.48437500, 0.61328125, 0.69921875, 0.71093750, 0.64843750],\n",
       "        [0.56250000, 0.69531250, 0.76953125, 0.75781250, 0.66796875],\n",
       "        [0.54687500, 0.66015625, 0.70312500, 0.67187500, 0.57421875],\n",
       "        [0.44140625, 0.51562500, 0.53125000, 0.48437500, 0.39843750]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qmodel.conv1.weight[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2bd68559-e3f0-4e8b-bfe6-fa8fa85eff59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ea933fbc-21e3-4890-abdf-c7561da1a157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 32])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0][0].size() # 1024개의 elements(32x32) 1차원으로 핀 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1742ae26-57f3-462e-8743-13781390d531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0][0][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cc669afc-cb3c-4d1b-9692-f2dc36b4b8a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.01798622,  0.08375905,  0.04467494, -0.02210203,  0.10713725,  0.04762135], requires_grad=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv1.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a37497b5-8b10-4ca2-a027-e84007a17d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.34375000, 0.44921875, 0.53125000, 0.55859375, 0.52343750],\n",
       "        [0.48437500, 0.61328125, 0.69921875, 0.71093750, 0.64843750],\n",
       "        [0.56250000, 0.69531250, 0.76953125, 0.75781250, 0.66796875],\n",
       "        [0.54687500, 0.66015625, 0.70312500, 0.67187500, 0.57421875],\n",
       "        [0.44140625, 0.51562500, 0.53125000, 0.48437500, 0.39843750]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qmodel.conv1.weight[0][0][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d91d7db-4cb1-4495-a3ab-b646ce4d7a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.34375000, 0.44921875, 0.53125000, 0.55859375, 0.52343750], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qmodel.conv1.weight[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6de24b4b-a722-46ae-a8da-ddac5dc37e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.48437500, 0.61328125, 0.69921875, 0.71093750, 0.64843750], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qmodel.conv1.weight[0][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "96db355f-942b-4549-83ab-1f536158dc80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.01562500,  0.08203125,  0.04296875, -0.01953125,  0.10546875,  0.04687500], requires_grad=True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qmodel.conv1.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b9fac0e5-6e84-498c-8569-4e6f3abe4571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.78333333333333"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluation(train_loader):\n",
    "    total, correct = 0, 0\n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(args.device), labels.to(args.device)\n",
    "        output = model(inputs)\n",
    "        max_pred, pred = torch.max(output.data, dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (pred == labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "evaluation(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4516603f-7f06-4013-b863-d0510b44f3a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.67"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluation(test_loader):\n",
    "    total, correct = 0, 0\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(args.device), labels.to(args.device)\n",
    "        output = model(inputs)\n",
    "        max_pred, pred = torch.max(output.data, dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (pred == labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "evaluation(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ce70955a-6bc5-409b-959b-423e7af6f5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_index, (image, label) in enumerate(test_loader):\n",
    "    image, label = image.to(args.device), label.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6493eb58-8f1e-438f-b16d-f4aee0e2fbeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 32, 32])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "db4e2e2c-7eb3-4f10-b190-fb0e0661cc3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 32, 32])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_input = image\n",
    "a_input.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772ef979-11c3-4f5e-b7f0-c9bee8136df4",
   "metadata": {},
   "source": [
    "# Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a15940b-2f74-4978-86b4-9521cdde2ac7",
   "metadata": {},
   "source": [
    "conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "81d78171-9b67-41dc-8b88-41533090b2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(X, filters,bias, stride=1, pad=0):\n",
    "    n, c, h, w = X.shape # 1, 1, 32, 32\n",
    "    n_f, _, filter_h, filter_w = filters.shape\n",
    "    \n",
    "    out_h = (h+2*pad-filter_h)//stride + 1\n",
    "    out_w = (w+2*pad-filter_w)//stride + 1\n",
    "    # add padding to height and width.\n",
    "    in_X = F.pad(X,(0,0,0,0,pad,pad,pad,pad),\"constant\", 0)\n",
    "    out  = torch.zeros((n, n_f, out_h, out_w))\n",
    "    \n",
    "    for i in range(n): # for each image.\n",
    "        for c in range(n_f): # for each channel.\n",
    "            for h in range(out_h): # slide the filter vertically.\n",
    "                h_start = h * stride\n",
    "                h_end = h_start + filter_h\n",
    "                for w in range(out_w): # slide the filter horizontally.\n",
    "                    w_start = w * stride\n",
    "                    w_end = w_start + filter_w\n",
    "                    # Element-wise multiplication.\n",
    "                    out[i, c, h, w] = torch.sum(in_X[i,:,h_start:h_end,w_start:w_end]*filters[c])+bias[c]\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26298221-319b-4203-b8a6-14e830ea4010",
   "metadata": {},
   "source": [
    "ReLU_4D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "af313172-fc36-431f-a77b-7180631e610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU_4D(X):\n",
    "    n, c, h, w = X.shape\n",
    "    \n",
    "    out = torch.zeros(n,c,h,w)\n",
    "    \n",
    "    for i in range(n): # for each image\n",
    "        for ch in range(c): # for each channel\n",
    "            for o_h in range(h): # for each height\n",
    "                for o_w in range(w): # for each width\n",
    "                    x = X[i, ch, o_h, o_w]\n",
    "                    if x > 0:\n",
    "                        out[i, ch, o_h, o_w] = x\n",
    "                    else:\n",
    "                        out[i, ch, o_h, o_w] = 0\n",
    "                        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb4ace7-4546-4b98-a339-2c6c3be02c1a",
   "metadata": {},
   "source": [
    "ReLU_2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cd6044c5-2c49-461e-97c6-cb9be961e4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU_2D(X):\n",
    "    n, c = X.shape\n",
    "    \n",
    "    out = torch.zeros(n, c)\n",
    "    \n",
    "    for i in range(n): # for each image\n",
    "        for ch in range(c): # for each channel\n",
    "            x = X[i, ch]\n",
    "            if x > 0:\n",
    "                out[i, ch] = x\n",
    "            else:\n",
    "                out[i, ch] = 0\n",
    "                \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7c115a-a9f7-4bb2-ad06-43cdbb7077d9",
   "metadata": {},
   "source": [
    "AvgPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bafee8c0-ba75-4607-9b41-01785478d9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AvgPool2d(X,kernel_size,stride,pad=0):\n",
    "    n, c, h, w = X.shape\n",
    "    ker_w, ker_h = kernel_size\n",
    "    \n",
    "    out_h = (h + 2*pad - ker_h)//stride + 1\n",
    "    out_w = (w + 2*pad - ker_w)//stride + 1\n",
    "    \n",
    "    out = torch.zeros(n,c,out_h,out_w)\n",
    "    for i in range(n) : #for each image\n",
    "        for ch in range(c) : #for each channel \n",
    "             for h in range(out_h) :\n",
    "                    h_start = h * stride\n",
    "                    h_end = h_start + ker_h\n",
    "                    for w in range(out_w):\n",
    "                        w_start = w * stride\n",
    "                        w_end = w_start + ker_w\n",
    "                        #element average\n",
    "                        out[i, ch, h, w] = torch.mean(X[i,ch,h_start:h_end,w_start:w_end])\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6722d89a-10e9-48b9-a413-5bcf81a7db80",
   "metadata": {},
   "source": [
    "# checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "10082148-c9dd-4a18-b650-aa6caef1ac95",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters1 = qmodel.conv1.weight\n",
    "filters2 = qmodel.conv2.weight\n",
    "filters3 = qmodel.conv3.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "69d8aa20-3739-4736-9a23-9065866095d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias1 = qmodel.conv1.bias\n",
    "bias2 = qmodel.conv2.bias\n",
    "bias3 = qmodel.conv3.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "09a26043-2ee2-4c0b-a147-3d13e4d60dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1_filters1 = qmodel.fc1.weight\n",
    "fc2_filters2 = qmodel.fc2.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ac5702cb-c327-45d1-bd4f-fb976280f33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1_bias1 = qmodel.fc1.bias\n",
    "fc2_bias2 = qmodel.fc2.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9341de09-e21b-4b04-aab3-c1b3e14dbfb4",
   "metadata": {},
   "source": [
    "act0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8a8a61e6-4de5-4f9b-9052-63ef7277e217",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_conv1 = conv(a_input, filters1, bias1, stride=1, pad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b6abd79b-1b58-47b8-bf46-1e6baf899d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_relu1 = ReLU_4D(layer_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "906b41a5-6a20-4c0b-ae8a-b66270c6cc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_avgpool1 = AvgPool2d(layer_relu1, (2,2),2,pad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b7992718-d16e-4d26-a9d1-4a70000a8aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 6, 14, 14])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_avgpool1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "319ed177-5f3a-4367-84ba-2e2f1c09af67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 6, 14, 14])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act0.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f96d4dba-d546-4590-852f-43fffe93235f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02734375 0.03125\n",
      "number of different value is : 1/18816\n"
     ]
    }
   ],
   "source": [
    "fix_act0 = torch.zeros(16,6,14,14)\n",
    "\n",
    "total = 0\n",
    "num = 0\n",
    "\n",
    "for a in range(16) :\n",
    "    for b in range(6) :\n",
    "        for c in range(14) :\n",
    "            for d in range(14) :\n",
    "                fix_act0[a][b][c][d] = flp2fix(layer_avgpool1[a][b][c][d],args.full_bits,args.frac_bits).fFull\n",
    "                if (torch.equal(fix_act0[a][b][c][d],act0[a][b][c][d])==False):\n",
    "                    print(fix_act0[a][b][c][d].item(), act0[a][b][c][d].item())\n",
    "                    num += 1\n",
    "                total += 1\n",
    "\n",
    "print(\"number of different value is : {num}/{total}\".format(num=num,total=total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e6867c2a-40a4-413a-872c-8c09c745eab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(fix_act0, act0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3207ec7c-3e0e-4937-bbda-79e12cb03465",
   "metadata": {},
   "source": [
    "act1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "05a3f781-ada8-4fdf-aafc-20e5661943ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_conv2 = conv(fix_act0, filters2, bias2, stride=1,pad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7e40c7a8-cfc8-49ad-ab85-4ab29300110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_relu2 = ReLU_4D(layer_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "384678a5-5168-4749-ab7c-21c3cdb1eeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_avgpool2 = AvgPool2d(layer_relu2, (2,2),2,pad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fdd898fd-92d5-42fa-90bf-91d57d027a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 16, 5, 5])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_avgpool2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "bc15e173-8d15-4aa9-9382-4aa40a794753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 16, 5, 5])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "adb20fdd-95be-422a-9c72-ce2b961b81f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of different value is : 0/6400\n"
     ]
    }
   ],
   "source": [
    "fix_act1 = torch.zeros(16,16,5,5)\n",
    "\n",
    "total = 0\n",
    "num = 0\n",
    "\n",
    "for a in range(16) :\n",
    "    for b in range(16) :\n",
    "        for c in range(5) :\n",
    "            for d in range(5) :\n",
    "                fix_act1[a][b][c][d] = flp2fix(layer_avgpool2[a][b][c][d],args.full_bits,args.frac_bits).fFull\n",
    "                if (torch.equal(fix_act1[a][b][c][d],act1[a][b][c][d])==False):\n",
    "                    num += 1\n",
    "                    print(fix_act1[a][b][c][d].item(), act1[a][b][c][d].item())\n",
    "                total += 1\n",
    "\n",
    "print(\"number of different value is : {num}/{total}\".format(num=num,total=total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fcfc531b-e0e8-460c-bd61-0faffbbaebd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(fix_act1, act1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5578ce-8c51-497c-b87c-50f62fe80cec",
   "metadata": {},
   "source": [
    "act2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "345fe442-525a-4b8b-ac71-b1614e9b8f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_conv3 = conv(fix_act1, filters3, bias3, stride=1,pad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9c0a4cd8-e209-402c-9361-2c87c95ad891",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_relu3 = ReLU_4D(layer_conv3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7699438c-2f69-4d25-9105-f62c641d358b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 120, 1, 1])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "38ab0103-569e-44be-9f4d-5cb4328da6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of different value is : 0/1920\n"
     ]
    }
   ],
   "source": [
    "fix_act2 = torch.zeros(16,120,1,1)\n",
    "\n",
    "total = 0\n",
    "num = 0\n",
    "\n",
    "for a in range(16) :\n",
    "    for b in range(120) :\n",
    "        for c in range(1) :\n",
    "            for d in range(1) :\n",
    "                fix_act2[a][b][c][d] = flp2fix(layer_relu3[a][b][c][d],args.full_bits,args.frac_bits).fFull\n",
    "                if (torch.equal(fix_act2[a][b][c][d],act2[a][b][c][d])==False):\n",
    "                    num += 1\n",
    "                    print(fix_act2[a][b][c][d].item(), act2[a][b][c][d].item())\n",
    "                total += 1\n",
    "\n",
    "print(\"number of different value is : {num}/{total}\".format(num=num,total=total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7887569b-8253-4427-9d17-06ea78cd71d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(fix_act2, act2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0f2d25-a23e-43be-a570-0e64b1a8f00e",
   "metadata": {},
   "source": [
    "act3(flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8c6955c4-0d53-41e4-8c44-752de38a30f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_flatten = fix_act2.view(16,120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1a3cf6fa-5271-48e8-bc26-010ef1505750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 120])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act3.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e0ee582a-7de7-4f4b-a962-743a2db1c8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of different value is : 0/1920\n"
     ]
    }
   ],
   "source": [
    "fix_act3 = torch.zeros(16,120)\n",
    "\n",
    "total = 0\n",
    "num = 0\n",
    "\n",
    "for a in range(16) :\n",
    "    for b in range(120) :\n",
    "        fix_act3[a][b] = flp2fix(layer_flatten[a][b],args.full_bits,args.frac_bits).fFull\n",
    "        if (torch.equal(fix_act3[a][b],act3[a][b])==False):\n",
    "            num += 1\n",
    "            print(fix_act3[a][b].item(), act3[a][b].item())\n",
    "        total += 1\n",
    "\n",
    "print(\"number of different value is : {num}/{total}\".format(num=num,total=total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "679a6032-5dcb-4dc1-bb29-3e02535eb636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(fix_act3, act3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3319ce-916b-4ab2-ab9e-4a4c62020919",
   "metadata": {},
   "source": [
    "act4(fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c81b46a6-851d-44ef-9e27-37b93bf7e1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_fc1 = torch.matmul(fix_act3, fc1_filters1.t()) + fc1_bias1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c864e113-3137-4f9a-8d57-3e53eda2045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_relu_fc1 = ReLU_2D(layer_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "11b5423b-0da4-4d88-8ef3-642dae2c3c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 84])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act4.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f926ee3e-f6f8-46ad-8987-8d8a73d9580d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of different value is : 0/1344\n"
     ]
    }
   ],
   "source": [
    "fix_act4 = torch.zeros(16,84)\n",
    "\n",
    "total = 0\n",
    "num = 0\n",
    "\n",
    "for a in range(16) :\n",
    "    for b in range(84) :\n",
    "        fix_act4[a][b] = flp2fix(layer_relu_fc1[a][b],args.full_bits,args.frac_bits).fFull\n",
    "        if (torch.equal(fix_act4[a][b],act4[a][b])==False):\n",
    "            num += 1\n",
    "            print(fix_act4[a][b].item(), act4[a][b].item())\n",
    "        total += 1\n",
    "\n",
    "print(\"number of different value is : {num}/{total}\".format(num=num,total=total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9e70b1a7-d8c8-4b7c-85d2-d9326547f885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(fix_act4, act4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c05b0b-3eda-4d53-9a98-ca7cded72dbd",
   "metadata": {},
   "source": [
    "act5(fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f892480e-1033-4f44-b787-b5f1712a8785",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_fc2 = torch.matmul(fix_act4, fc2_filters2.t()) + fc2_bias2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c8de35e7-9e72-441d-a2de-6e4e06d2218f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 10])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act5.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cb8c0a2c-d0c4-4874-8e0d-bd240b8baa57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of different value is : 0/160\n"
     ]
    }
   ],
   "source": [
    "fix_act5 = torch.zeros(16,10)\n",
    "\n",
    "total = 0\n",
    "num = 0\n",
    "\n",
    "for a in range(16) :\n",
    "    for b in range(10) :\n",
    "        fix_act5[a][b] = flp2fix(layer_fc2[a][b],args.full_bits,args.frac_bits).fFull\n",
    "        if (torch.equal(fix_act5[a][b],act5[a][b])==False):\n",
    "            num += 1\n",
    "            print(fix_act5[a][b].item(), act5[a][b].item())\n",
    "        total += 1\n",
    "\n",
    "print(\"number of different value is : {num}/{total}\".format(num=num,total=total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ebd29e24-e8af-4341-8b54-947788d594a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(fix_act5, act5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
