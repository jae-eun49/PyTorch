{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c688ae26-bef9-4988-9133-0346dc4eff32",
   "metadata": {},
   "source": [
    "# Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5075e9a-43ff-49f5-888b-342d5ecda78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import tqdm\n",
    "from tqdm import tqdm_notebook as tq\n",
    "import os, time, math, copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_printoptions(precision=8, linewidth=50000)\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff365a5-2a3c-4c10-96db-c2d798de3517",
   "metadata": {},
   "source": [
    "# Print Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8afcf2b-07ca-4b03-93e7-89634b8b20e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLACK\t= '\\033[30m'\n",
    "RED\t\t= '\\033[31m'\n",
    "GREEN\t= '\\033[32m'\n",
    "YELLOW\t= '\\033[33m'\n",
    "BLUE\t= '\\033[34m'\n",
    "MAGENTA\t= '\\033[35m'\n",
    "CYAN\t= '\\033[36m'\n",
    "RESET\t= '\\033[0m'\n",
    "SEL\t\t= '\\033[7m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d6f449a-b38f-449a-abfc-dbdfffd65a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class\tfxp:\n",
    "\tdef\t__init__(self, bIn, iBWF):\n",
    "\t\tself.iFullBW\t= len(bIn)\n",
    "\t\tself.iIntgBW\t= self.iFullBW - iBWF\n",
    "\t\tself.bSign\t\t= bIn[0]\n",
    "\t\tself.bIntg\t\t= bIn[:self.iIntgBW]\n",
    "\t\tself.bFrac\t\t= bIn[self.iIntgBW:]\n",
    "\t\tself.fFull\t\t= 0\n",
    "\t\ttry:\n",
    "\t\t\tfor idx, bit in enumerate(bIn):\n",
    "\t\t\t\tif\tidx == 0:\n",
    "\t\t\t\t\tself.fFull = self.fFull + int(bit,2) * -pow(2, self.iIntgBW - 1)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tself.fFull = self.fFull + int(bit,2) * pow(2, self.iIntgBW - 1 - idx)\n",
    "\t\texcept:\n",
    "\t\t\tprint(bIn)\n",
    "\t\tself.dispFull\t= RED + self.bIntg + BLUE + self.bFrac + RESET\n",
    "\t\treturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "610b93d9-8619-4bcc-b29d-9a3d35104bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class\tflp2fix:\n",
    "\tdef\t__init__(self, fIn, iBW, iBWF):\n",
    "\t\tself.fMin\t\t= - 2 ** (iBW - iBWF - 1)\n",
    "\t\tself.fMax\t\t= (2 ** (iBW-1) - 1) * (2 ** -iBWF)\n",
    "\t\tself.fResol\t\t= 2 ** -iBWF\n",
    "\t\tif fIn < self.fMin or fIn > self.fMax:\n",
    "\t\t\tprint(f'({fIn}): Out of input range ({self.fMax}/{self.fMin}) during flp -> fix converting ')\n",
    "\t\tself.iBW\t\t= iBW\n",
    "\t\tself.iBWI\t\t= iBW - iBWF\n",
    "\t\tself.iBWF\t\t= iBWF\n",
    "\n",
    "\t\tself.iFLP2INT\t= abs(int(fIn * 2 ** iBWF))\n",
    "\t\tif fIn < 0:\n",
    "\t\t\tself.iFLP2INT = 2 ** (iBW-1) - self.iFLP2INT\n",
    "\n",
    "\t\tif fIn >= 0:\n",
    "\t\t\tself.bFull = bin(self.iFLP2INT)[2:].rjust(iBW, '0')\n",
    "\t\telse:\n",
    "\t\t\tself.bFull = '1'+bin(self.iFLP2INT)[2:].rjust(iBW-1, '0')\n",
    "\t\t\tif len(self.bFull) > iBW:\n",
    "\t\t\t\tself.bFull = '0' * iBW\n",
    "\n",
    "\t\tself.cssFxp\t\t= fxp(self.bFull, self.iBWF)\n",
    "\t\tself.bSign\t\t= self.cssFxp.bSign\n",
    "\t\tself.bIntg\t\t= self.cssFxp.bIntg\n",
    "\t\tself.bFrac\t\t= self.cssFxp.bFrac\n",
    "\t\tself.fFull\t\t= self.cssFxp.fFull\n",
    "\t\treturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48099f0b-1b79-482e-8ec7-e978b1909645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def\tflp2fixTensor(fIn, iBW, iBWF):\n",
    "# \tfMin = - 2 ** (iBW - iBWF - 1)\n",
    "# \tfMax = (2 ** (iBW-1) - 1) * (2 ** -iBWF)\n",
    "# \tfTensor = fIn * (2 ** iBWF)\n",
    "# \tfTensor = fTensor.round() * (2 ** -iBWF)\n",
    "# \tif (fTensor.min() < fMin or fMax < fTensor.max()):\n",
    "# \t\tprint(f'fMin: {fMin}, fMax: {fMax}, fTensor.Min:{fTensor.min()}, fTensor.Max:{fTensor.max()}')\n",
    "# \treturn fTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db9ba737-a911-4652-a369-267585a231c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def\tflp2fixTensor(fIn, iBW, iBWF):\n",
    "\tfMin = - 2 ** (iBW - iBWF - 1)\n",
    "\tfMax = (2 ** (iBW-1) - 1) * (2 ** -iBWF)\n",
    "\tfList = []\n",
    "\tfor aTensor in fIn.view(-1):\n",
    "\t\tfList.append(flp2fix(aTensor, iBW, iBWF).fFull)\n",
    "\treturn torch.tensor(fList).view(fIn.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b68ddb-8ef2-42a1-94c1-4f708b4907d7",
   "metadata": {},
   "source": [
    "# User Define Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "470d6312-f557-4f84-899f-95686b41e1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '~/dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002c0a67-1a39-4ce3-913b-749c11f6878b",
   "metadata": {},
   "source": [
    "# Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa129a9b-9a85-4de6-b42a-f8cbfc31231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch for MNIST dataset')\n",
    "parser.add_argument('--device', type=str, default='cpu', help='Device')\n",
    "parser.add_argument('--shuffle', action='store_true', default=False, help='enables data shuffle')\n",
    "parser.add_argument('--dataset', type=str, default='mnist', help='training dataset')\n",
    "parser.add_argument('--data_path', type=str, default=data_path, help='path to MNIST')\n",
    "parser.add_argument('--batch_size', type=int, default=64, help='batch size')\n",
    "parser.add_argument('--epochs', type=int, default=10, help='number of epochs to train')\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='learning rate')\n",
    "parser.add_argument('--optimizer', type=str, default='adam', help='optimizer')\n",
    "parser.add_argument('--loss_func', type=str, default='cel', help='optimizer')\n",
    "parser.add_argument('--quant_opt', type=str, default='asym', help='Type of Quantization')\n",
    "parser.add_argument('--full_bits', type=int, default=16, help='Number of Quantization Bits')\n",
    "parser.add_argument('--frac_bits', type=int, default=8, help='Number of Quantization Bits')\n",
    "parser.add_argument('--pretrained', type=bool, default=True, help='Pretrained Model')\n",
    "parser.add_argument('--act_quant', type=bool, default=False, help='Activation Quantization')\n",
    "parser.add_argument('--disp', type=bool, default=False, help='Display Model Information')\n",
    "\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87afa21c-b407-4698-bf0b-85e9dc4c94f1",
   "metadata": {},
   "source": [
    "# Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c82ac1a-0065-4d3f-a915-a01a91eb47a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.device == 'cuda' else {}\n",
    "data_transform = transforms.Compose([transforms.Resize((32,32)),transforms.ToTensor()])\n",
    "if args.dataset == 'mnist':\n",
    "\ttrain_loader = torch.utils.data.DataLoader(\n",
    "\t\tdataset=datasets.MNIST(\n",
    "\t\t\troot=args.data_path,\n",
    "\t\t\ttrain=True,\n",
    "\t\t\tdownload=True,\n",
    "\t\t\ttransform=data_transform\n",
    "\t\t),\n",
    "\t\tbatch_size=args.batch_size,\n",
    "\t\tshuffle=args.shuffle,\n",
    "\t\t**kwargs\n",
    "\t)\n",
    "\n",
    "\ttest_loader = torch.utils.data.DataLoader(\n",
    "\t\tdataset=datasets.MNIST(\n",
    "\t\t\troot=args.data_path,\n",
    "\t\t\ttrain=False,\n",
    "\t\t\tdownload=True,\n",
    "\t\t\ttransform=data_transform\n",
    "\t\t),\n",
    "\t\tbatch_size=args.batch_size,\n",
    "\t\tshuffle=args.shuffle,\n",
    "\t\t**kwargs\n",
    "\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625782c6-b278-4485-a8c9-7c6fec5aafdf",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98354784-1637-4f2c-85b2-6b53f0805bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class MLP(nn.Module):\n",
    "#\tdef __init__(self):\n",
    "#\t\tsuper(MLP, self).__init__()\n",
    "#\t\tself.flatten = nn.Flatten()\n",
    "#\t\tself.fc1 = nn.Linear(28*28, 16)\n",
    "#\t\tself.relu1 = nn.ReLU()\n",
    "#\t\tself.fc2 = nn.Linear(16, 16)\n",
    "#\t\tself.relu2 = nn.ReLU()\n",
    "#\t\tself.fc3 = nn.Linear(16, 10)\n",
    "\t\t\n",
    "#\tdef forward(self, x):\n",
    "#\t\tx = self.flatten(x)\n",
    "#\t\tx = self.fc1(x)\n",
    "#\t\tx = self.relu1(x)\n",
    "#\t\tx = self.fc2(x)\n",
    "#\t\tx = self.relu2(x)\n",
    "#\t\tlogits = self.fc3(x)\n",
    "#\t\treturn logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fe7be2b-c72c-4c32-881a-e32b82f3345f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet_5(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc2): Linear(in_features=84, out_features=10, bias=True)\n",
      "  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (act): Tanh()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LeNet_5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet_5,self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1)\n",
    "        self.conv3 = nn.Conv2d(16, 120, kernel_size=5, stride=1)  # Flatten layer\n",
    "        self.fc1 = nn.Linear(120, 84)\n",
    "        self.fc2 = nn.Linear(84, 10)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.act = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.conv1(x))\n",
    "        x = F.avg_pool2d(x, 2, 2)\n",
    "        x = F.tanh(self.conv2(x))\n",
    "        x = F.avg_pool2d(x, 2, 2)\n",
    "        x = F.tanh(self.conv3(x))\n",
    "        x = self.flatten(x)\n",
    "#        x = x.view(-1, 120) # Flatten layer\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = LeNet_5()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ebfd9aa-9e7b-4638-8db1-101093cdfb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genOptimizer(model, args):\n",
    "\tif args.optimizer == 'sgd':\n",
    "\t\toptimizer = torch.optim.SGD(model.parameters(), lr=args.lr)\n",
    "\tif args.optimizer == 'adam':\n",
    "\t\toptimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\treturn optimizer\n",
    "\n",
    "def genLossFunc(args):\n",
    "\tif args.loss_func == 'cel':\n",
    "\t\tloss_func = nn.CrossEntropyLoss()\n",
    "\treturn loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb8fe687-70b1-4f29-8dd4-316acae35dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, epoch, args):\n",
    "\tmodel.train()\n",
    "\tloss_func = genLossFunc(args)\n",
    "\toptimizer = genOptimizer(model, args)\n",
    "\tmax_batch_index = int(np.floor(len(train_loader.dataset)/args.batch_size))\n",
    "\trunning_loss = 0\n",
    "\tfor batch_index, (image, label) in enumerate(tq(train_loader, desc='Train', leave=False)):\n",
    "\t\timage, label = image.to(args.device), label.to(args.device)\n",
    "\t\tpred = model(image)\n",
    "\t\tloss = loss_func(pred, label)\n",
    "\t\trunning_loss += loss.item()#*image.size(0)\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\t\t\t\n",
    "\tprint(f'Epoch {epoch+1:<3d}: Avg. Loss: {running_loss/len(train_loader.dataset):.4f}', end = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29189102-050a-463a-bbb8-18ca4ab3700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, model, args):\n",
    "\tmodel.eval()\n",
    "\twith torch.no_grad():\n",
    "\t\tloss_func = genLossFunc(args)\n",
    "\t\tloss, correct = 0, 0\n",
    "# \t\tfor batch_index, (image, label) in enumerate(tq(test_loader, desc='Test', leave=False)):\n",
    "\t\tfor batch_index, (image, label) in enumerate(test_loader):\n",
    "\t\t\timage, label = image.to(args.device), label.to(args.device)\n",
    "\t\t\tpred = model(image)\n",
    "\t\t\tloss += loss_func(pred, label).item()#*image.size(0)\n",
    "\t\t\tcorrect += (pred.argmax(1) == label).type(torch.int).sum().item()\n",
    "\tloss /= len(test_loader.dataset)\n",
    "\tcorrect_rate = 100 * correct / len(test_loader.dataset)\n",
    "\tprint(f'Accuracy: {correct}/{len(test_loader.dataset)} ({correct_rate:>.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec4a2872-aab5-491a-8b9c-53861468e10d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  : Avg. Loss: 0.0047\tAccuracy: 9598/10000 (96.0%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  : Avg. Loss: 0.0016\tAccuracy: 9723/10000 (97.2%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3  : Avg. Loss: 0.0011\tAccuracy: 9768/10000 (97.7%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4  : Avg. Loss: 0.0008\tAccuracy: 9786/10000 (97.9%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5  : Avg. Loss: 0.0006\tAccuracy: 9767/10000 (97.7%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6  : Avg. Loss: 0.0005\tAccuracy: 9806/10000 (98.1%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7  : Avg. Loss: 0.0004\tAccuracy: 9787/10000 (97.9%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8  : Avg. Loss: 0.0003\tAccuracy: 9813/10000 (98.1%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9  : Avg. Loss: 0.0003\tAccuracy: 9781/10000 (97.8%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 : Avg. Loss: 0.0002\tAccuracy: 9784/10000 (97.8%)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def main(model):\n",
    "\tfor epoch in range(args.epochs):\n",
    "\t\ttrain(train_loader, model, epoch, args)\n",
    "\t\ttest(test_loader, model, args)\n",
    "\tprint(\"Done!\")\n",
    "\treturn model\n",
    "\n",
    "model = main(LeNet_5().to(args.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f33b1a6e-c60e-4778-8764-6d56ef45b8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 9592/10000 (95.9%)\n"
     ]
    }
   ],
   "source": [
    "if args.pretrained:\n",
    "\tmodel = LeNet_5().to(args.device)\n",
    "\tmodel.load_state_dict(torch.load('PTQ_LeNet.pth'))\n",
    "\ttest(test_loader, model, args)\n",
    "else:\n",
    "\tmodel = main(LeNet_5().to(args.device))\n",
    "\ttorch.save(model.state_dict(), 'PTQ_LeNet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cd9ecd0-0247-4bde-bbb9-2a5c2478c246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model2fix(model, args):\n",
    "\tfor name, _ in model.named_parameters():\n",
    "\t\texec(f'model.{name}.data = flp2fixTensor(model.{name}.data, {args.full_bits}, {args.frac_bits})')\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "456938c0-6707-4718-85e2-0895fd6e0f22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def in2fix(images, full_width=args.full_bits, frac_width=args.frac_bits):\n",
    "# \tdim_images = images.size()\n",
    "# \timages = images.view(-1)\n",
    "# \tfor idx_image, image in enumerate(images):\n",
    "# \t\ttemp_css = flp2fix(image, full_width, frac_width)\n",
    "# \t\timages[idx_image] = torch.tensor(temp_css.fFull)\n",
    "# \t\tdel temp_css\n",
    "# \treturn images.view(dim_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebfd12bd-736c-4f63-af51-25495477c998",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def quantFixForward(model, x, args):\n",
    "    cmodel = copy.deepcopy(model).to(args.device)\n",
    "    with torch.no_grad():\n",
    "\n",
    "        act0 = cmodel.pool(cmodel.act(cmodel.conv1(x)))\n",
    "        act0 = flp2fixTensor(act0, args.full_bits, args.frac_bits)\n",
    "        \n",
    "        act1 = cmodel.pool(cmodel.act(cmodel.conv2(act0)))\n",
    "        act1 = flp2fixTensor(act1, args.full_bits, args.frac_bits)\n",
    "        \n",
    "        act2 = cmodel.act(cmodel.conv3(act1))\n",
    "        act2 = flp2fixTensor(act2, args.full_bits, args.frac_bits)\n",
    "\n",
    "        act3 = cmodel.flatten(act2)\n",
    "        act3 = flp2fixTensor(act3, args.full_bits, args.frac_bits)\n",
    "\t\t\n",
    "        act4 = cmodel.act(cmodel.fc1(act3))\n",
    "        act4 = flp2fixTensor(act4, args.full_bits, args.frac_bits)\n",
    "        \n",
    "        act5 = cmodel.fc2(act4)\n",
    "        act5 = flp2fixTensor(act5, args.full_bits, args.frac_bits)\n",
    "        \n",
    "    return cmodel, act0, act1, act2, act3, act4, act5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "22f22fd2-621a-4878-b4da-9ec317de3788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 6, 14, 14])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act0.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "1593f5d3-594f-4dd1-988f-ae14e36e0828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 6, 1, 1])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "98acc3eb-9306-4937-89e0-ad877de20cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 120, 1, 1])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "61a10326-c887-4113-b995-e02e48867941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 120])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act3.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7e6eb891-baa0-414e-8094-830138c315cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 84])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act4.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "62e6619c-a0a5-4f50-91fd-6a3990926cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 10])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act5.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d35ae3-970e-4386-9e03-86690e48af4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testQuant(model, test_loader, args):\n",
    "\tqmodel = copy.deepcopy(model).to(args.device)\n",
    "\tqmodel = model2fix(qmodel, args)\n",
    "\tqmodel.eval()\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tloss_func = genLossFunc(args)\n",
    "\t\tloss, correct = 0, 0\n",
    "\t\tfor batch_index, (image, label) in enumerate(test_loader):\n",
    "\t\t\timage, label = image.to(args.device), label.to(args.device)\n",
    "\t\t\tqmodel, act0, act1, act2, act3, act4, act5  = quantFixForward(qmodel, image, args)\n",
    "\t\t\ty = act5\n",
    "\t\t\tloss += loss_func(y, label).item()#*image.size(0)\n",
    "\t\t\tcorrect += (y.argmax(1) == label).type(torch.int).sum().item()\n",
    "\tcorrect_rate = 100 * correct / len(test_loader.dataset)\n",
    "\tprint(f'Accuracy: {correct}/{len(test_loader.dataset)} ({correct_rate:>.1f}%) Loss: {loss/len(test_loader.dataset):.2f}')\n",
    "\treturn qmodel, act0, act1, act2, act3, act4, act5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab11ff5-8eea-4928-85a9-b92145de83d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 9585/10000 (95.8%) Loss: 0.00\n"
     ]
    }
   ],
   "source": [
    "qmodel, act0, act1, act2, act3, act4, act5 = testQuant(model, test_loader, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2ffe05-2a11-431d-acae-ba497de3f53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractParams(model, args):\n",
    "    for key in model.state_dict().keys():\n",
    "        layer_name = key.split('.')[0]\n",
    "        param_type = 'w' if 'weight' in key else 'b'\n",
    "        for idx, params in enumerate(eval(f'qmodel.{key}.data')):\n",
    "        #    print(idx)\n",
    "        #    print(params.dim())\n",
    "            with open(f'./mif/{layer_name}_{param_type}_{idx}.mif', 'w') as fh:\n",
    "                if param_type == 'w':\n",
    "                    if params.dim() == 3 :\n",
    "                        for idx, dim1 in enumerate(params):\n",
    "                            for idx, dim2 in enumerate(dim1):\n",
    "                                for idx, param in enumerate(dim2):\n",
    "                                    bin_param = flp2fix(param, args.full_bits, args.frac_bits).bFull\n",
    "                                    fh.write(bin_param + ('\\n','')[idx == len(params)-1])\n",
    "                    elif params.dim() == 1: # fc2 layer\n",
    "                        for idx, param in enumerate(params):\n",
    "                            bin_param = flp2fix(param, args.full_bits, args.frac_bits).bFull\n",
    "                            fh.write(bin_param + ('\\n', '')[idx == len(params)-1])\n",
    "                    \n",
    "                #    for idx, param in enumerate(params):\n",
    "                #        bin_param = flp2fix(param, args.full_bits, args.frac_bits).bFull\n",
    "                #        fh.write(bin_param + ('\\n','')[idx == len(params)-1])\n",
    "                else:\n",
    "                    bin_param = flp2fix(params, args.full_bits, args.frac_bits).bFull\n",
    "                    fh.write(bin_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eabfcc-a77d-4f0a-ad83-13d56f42a664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genInputVector(test_loader, args):\n",
    "\tout_path = './vec'\n",
    "\tos.system(f'rm -rf {out_path};mkdir -p {out_path}')\n",
    "\twith open(f'{out_path}/labels.vec', 'w') as fh_labels:\n",
    "\t\twith open(f'{out_path}/images.vec', 'w') as fh_images:\n",
    "\t\t\tfor batch_index, (images, labels) in enumerate(test_loader):\n",
    "\t\t\t\tfor (image, label) in zip(images, labels):\n",
    "\t\t\t\t\tbin_label = flp2fix(label, args.full_bits, 0).bFull\n",
    "\t\t\t\t\tfh_labels.write(bin_label+'\\n')\n",
    "\t\t\t\t\tfor pixel in image.view(-1):\n",
    "\t\t\t\t\t\tbin_pixel = flp2fix(pixel, args.full_bits, args.frac_bits).bFull\n",
    "\t\t\t\t\t\tfh_images.write(bin_pixel+'\\n')\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85911b39-7f25-4e1f-8faa-bd3d8632e6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if not args.pretrained:\n",
    "extractParams(model, args)\n",
    "genInputVector(test_loader, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8310847e-5a81-4649-a092-86466d06bdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = datasets.MNIST(\troot=args.data_path,\n",
    "\t\t\t\t\t\t\ttrain=False,\n",
    "\t\t\t\t\t\t\tdownload=True,\n",
    "\t\t\t\t\t\t\ttransform=data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9cc749c5-67ad-4d00-8512-8e7f4390716f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: /home/jaeeun/dataset\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(32, 32), interpolation=PIL.Image.BILINEAR)\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "702beea0-7a66-4f44-b0b8-5e9dc5314bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0][0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "250d7de6-54f2-4019-bf3b-ecea23df6462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 28, 28])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0f5e566-ed4d-472b-ac43-3586b95f5e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = iter(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2627d86d-b7d6-4449-ba38-567a9eb85cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labs = next(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e490a48b-9c69-4e1b-99bf-64daf58694a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5, 4, 0, 7, 4, 0, 1, 3, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labs[0:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d4073376-555b-4486-8106-e6074e0671a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0330ba5d-d615-4a11-820c-914ec3ad16f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.06250000, -0.01562500,  0.05859375, -0.11328125,  0.01171875,  0.05859375, -0.01171875, -0.06640625], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qmodel.fc1.weight[0][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71494c29-cd7d-4fa2-b543-f3ca1435ee35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 0.18045330,  0.19414586,  0.20404883,  0.24982135,  0.33966962],\n",
       "          [ 0.31025395,  0.31378302,  0.30601561,  0.33577812,  0.41172349],\n",
       "          [ 0.33776858,  0.35133591,  0.34539074,  0.35325623,  0.37955925],\n",
       "          [ 0.30402496,  0.32960871,  0.33463514,  0.32065710,  0.29119423],\n",
       "          [ 0.27029982,  0.30737719,  0.30560264,  0.24157465,  0.13625953]]],\n",
       "\n",
       "\n",
       "        [[[-0.21190982,  0.07295746, -0.23887797, -0.04612822, -0.34444273],\n",
       "          [-0.24775240, -0.20183615, -0.15598902, -0.19940718, -0.27916923],\n",
       "          [ 0.14290810, -0.01709873, -0.01640831, -0.19280452, -0.11677750],\n",
       "          [ 0.03226719,  0.02781692,  0.05474370,  0.12571493, -0.17665192],\n",
       "          [ 0.22604902,  0.31695715,  0.31795967, -0.04907313,  0.13301644]]],\n",
       "\n",
       "\n",
       "        [[[ 0.37560281,  0.34097508,  0.23746356,  0.12254444,  0.01867356],\n",
       "          [ 0.01831674, -0.03290712,  0.07245599, -0.05952620,  0.01810361],\n",
       "          [-0.18861026, -0.11805234,  0.04513501, -0.26250026, -0.28978369],\n",
       "          [-0.16496065, -0.27920553, -0.23232464, -0.21345389, -0.10761891],\n",
       "          [ 0.01987162, -0.08982264, -0.19125563, -0.30142349, -0.18631959]]],\n",
       "\n",
       "\n",
       "        [[[ 0.05445870,  0.30622530,  0.09806398, -0.12702924,  0.04032262],\n",
       "          [ 0.18140458,  0.04445156, -0.12661517, -0.23037255, -0.29664400],\n",
       "          [-0.21335918, -0.11502819, -0.18636329, -0.35353389, -0.33235058],\n",
       "          [ 0.17727979, -0.23403983, -0.07806525, -0.37234846, -0.10056955],\n",
       "          [ 0.07753071, -0.12306163, -0.06769233, -0.08976594,  0.08361164]]],\n",
       "\n",
       "\n",
       "        [[[-0.25406942, -0.29773536, -0.16317238,  0.02500228, -0.30292347],\n",
       "          [-0.08720774, -0.01172481, -0.05262516,  0.03268725,  0.10275773],\n",
       "          [-0.24787778, -0.02431200, -0.09295426, -0.00856282,  0.05007251],\n",
       "          [-0.01401406,  0.14776187,  0.38085908,  0.38956228,  0.32207093],\n",
       "          [-0.03968269, -0.03619498,  0.31393331,  0.41533205,  0.37884057]]],\n",
       "\n",
       "\n",
       "        [[[-0.29123506, -0.07523851, -0.09292636, -0.10730440, -0.21419880],\n",
       "          [-0.01150915,  0.14433193, -0.07211021, -0.14030287, -0.15932848],\n",
       "          [ 0.02128696,  0.09103486,  0.08054659,  0.24071626,  0.31623119],\n",
       "          [ 0.26760691,  0.32131398,  0.31844091,  0.01331831,  0.03077314],\n",
       "          [ 0.25382641,  0.07853169,  0.32594007,  0.21441920,  0.32010618]]]], requires_grad=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99b2a232-9693-4ebb-a46a-2c5096c59fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.17968750, 0.19140625, 0.20312500, 0.24609375, 0.33593750],\n",
       "        [0.30859375, 0.31250000, 0.30468750, 0.33203125, 0.41015625],\n",
       "        [0.33593750, 0.34765625, 0.34375000, 0.35156250, 0.37890625],\n",
       "        [0.30078125, 0.32812500, 0.33203125, 0.32031250, 0.28906250],\n",
       "        [0.26953125, 0.30468750, 0.30468750, 0.23828125, 0.13281250]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qmodel.conv1.weight[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2bd68559-e3f0-4e8b-bfe6-fa8fa85eff59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea933fbc-21e3-4890-abdf-c7561da1a157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 32])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0][0].size() # 1024개의 elements(32x32) 1차원으로 핀 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1742ae26-57f3-462e-8743-13781390d531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0][0][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc669afc-cb3c-4d1b-9692-f2dc36b4b8a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.80548680,  0.57787883,  0.04906479,  0.55835277, -0.27133316, -0.16827926], requires_grad=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv1.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a37497b5-8b10-4ca2-a027-e84007a17d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.17968750, 0.19140625, 0.20312500, 0.24609375, 0.33593750],\n",
       "        [0.30859375, 0.31250000, 0.30468750, 0.33203125, 0.41015625],\n",
       "        [0.33593750, 0.34765625, 0.34375000, 0.35156250, 0.37890625],\n",
       "        [0.30078125, 0.32812500, 0.33203125, 0.32031250, 0.28906250],\n",
       "        [0.26953125, 0.30468750, 0.30468750, 0.23828125, 0.13281250]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qmodel.conv1.weight[0][0][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1d91d7db-4cb1-4495-a3ab-b646ce4d7a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.17968750, 0.19140625, 0.20312500, 0.24609375, 0.33593750], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qmodel.conv1.weight[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6de24b4b-a722-46ae-a8da-ddac5dc37e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.30859375, 0.31250000, 0.30468750, 0.33203125, 0.41015625], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qmodel.conv1.weight[0][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "96db355f-942b-4549-83ab-1f536158dc80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.80468750,  0.57421875,  0.04687500,  0.55468750, -0.26953125, -0.16796875], requires_grad=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qmodel.conv1.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b9fac0e5-6e84-498c-8569-4e6f3abe4571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.585"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluation(train_loader):\n",
    "    total, correct = 0, 0\n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(args.device), labels.to(args.device)\n",
    "        output = model(inputs)\n",
    "        max_pred, pred = torch.max(output.data, dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (pred == labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "evaluation(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4516603f-7f06-4013-b863-d0510b44f3a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.92"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluation(test_loader):\n",
    "    total, correct = 0, 0\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(args.device), labels.to(args.device)\n",
    "        output = model(inputs)\n",
    "        max_pred, pred = torch.max(output.data, dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (pred == labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "evaluation(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b49083-cbb6-4fef-8fd6-7ff2567b8ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
